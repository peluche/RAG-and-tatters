{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG and tatters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threadbare implementation of RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ipywidgets html2text langchain langchain_community sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch as t\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    return html2text.html2text(str(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope():\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    def get_embedding(text):\n",
    "        return t.tensor(embeddings.embed_query(text))\n",
    "    return get_embedding\n",
    "\n",
    "get_embedding = scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to ingest full documents, another is to chunk them into smaller pieces. Let's compare different schemes of document chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = wikipedia_to_markdown('https://en.wikipedia.org/wiki/Magician_(fantasy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into fixed size chunks. For simplicity I'll fix the size in characters, but it would be smarter to split by amount of tokens instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magici',\n",
       " 'an_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disamb',\n",
       " 'iguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n\"Wizard (fantasy)\" redirects here. For other uses, see [Wiza',\n",
       " 'rd\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambiguation\\\\)\").\\n\\n[![](//upload.wi']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_fixed_size(text, size):\n",
    "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
    "\n",
    "chunks = chunk_fixed_size(document, 100)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magici',\n",
       " 'iki/Magician_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Mag',\n",
       " '(/wiki/Magi_\\\\(disambiguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n\"Wizard (fantasy)\" redirects here. For o',\n",
       " 'ere. For other uses, see [Wizard\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambi']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_fixed_size_overlap(text, size, overlap):\n",
    "    return [text[i:i+size] for i in range(0, len(text), size - overlap)]\n",
    "\n",
    "chunks = chunk_fixed_size_overlap(document, 100, 10)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive character split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on a hierarchy of specific landmarks (e.g. `'\\n\\n'`, `'\\n'`, `' '`) until we reach the desired size. This is meant to preserve more structure than simple fixed size split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\n',\n",
       " 'For other uses, see [Magician\\n(disambiguation)](/wiki/Magician_\\\\(disambiguation\\\\) \"Magician\\n',\n",
       " '\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disambiguation\\\\)\\n',\n",
       " '\"Magi \\\\(disambiguation\\\\)\").\\n\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_recursive_character_split(text, size, separators=['\\n\\n', '\\n', ' ']):\n",
    "    if len(text) <= size: return [text]\n",
    "    for separator in separators + ['']:\n",
    "        if (index := text[:size].rfind(separator)) != -1:\n",
    "            index += len(separator)\n",
    "            return [text[:index]] + chunk_recursive_character_split(text[index:], size, separators)\n",
    "\n",
    "chunks = chunk_recursive_character_split(document, 100)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document specific splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split based on the document specific grammar (e.g. markdown, HTML, PDF ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\n# The Enigmatic Life of Wizard Eldrath\\n',\n",
       " '\\n## Introduction',\n",
       " '\\nEldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his',\n",
       " ' mysterious powers and secretive nature.\\n',\n",
       " '\\n## Notable Achievements',\n",
       " '\\nEldrath is known for many great deeds, including the discovery of the lost city of Aranthar and',\n",
       " ' the creation of the spell of eternal light.\\n',\n",
       " '```\\ndef eternal_light():\\n    while True:\\n        light_spell.cast()\\n',\n",
       " '```\\nWhich remains one of the most powerful enchantments in existence.\\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_markdown(text, size, offset=0):\n",
    "    ''' piggyback on recursive character split for the demo but it should use a markdown parser '''\n",
    "    separators = [\n",
    "        '\\n# ', '\\n## ', '\\n### ', '\\n#### ', '\\n##### ', '\\n###### ', # headings\n",
    "        '```\\n', '\\n\\n', # blocks\n",
    "        '\\n', '`', '[', ']', '(', ')', '*', '_', # inline\n",
    "        ' ', # words\n",
    "    ]\n",
    "    if len(text) <= size: return [text]\n",
    "    for separator in separators + ['']:\n",
    "        if (index := text[offset:size].rfind(separator)) != -1:\n",
    "            index += offset\n",
    "            return [text[:index]] + chunk_markdown(text[index:], size, offset=len(separator))\n",
    "\n",
    "doc = '''\n",
    "# The Enigmatic Life of Wizard Eldrath\n",
    "\n",
    "## Introduction\n",
    "Eldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his mysterious powers and secretive nature.\n",
    "\n",
    "## Notable Achievements\n",
    "Eldrath is known for many great deeds, including the discovery of the lost city of Aranthar and the creation of the spell of eternal light.\n",
    "```\n",
    "def eternal_light():\n",
    "    while True:\n",
    "        light_spell.cast()\n",
    "```\n",
    "Which remains one of the most powerful enchantments in existence.\n",
    "'''\n",
    "chunks = chunk_markdown(doc, 100)\n",
    "chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semantic splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the document based on meaning. A naive aproach is to split the document into sentences (here I'll re-use recursive character split) compute their embeddings. Use the embeddings to find topics boundary in the text and merge the rest together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_chunks(chunks, indices, size=500):\n",
    "    ''' cluster chunks such that:\n",
    "      - each cluster is smaller or equal to size\n",
    "      - chunks are clustered according to their relative similarities\n",
    "    '''\n",
    "    indices = [i + 1 for i in indices] # shift to the right\n",
    "\n",
    "    def rec(start, end, idx=0):\n",
    "        # shortcircuit if the entire chunk fits\n",
    "        if sum(len(c) for c in chunks[start:end]) <= size:\n",
    "            return [''.join(chunks[start:end])]\n",
    "        for i in range(idx, len(indices)):\n",
    "            index = indices[i]\n",
    "            if start < index < end:\n",
    "                return rec(start, index, i + 1) + rec(index, end, i + 1)\n",
    "    \n",
    "    return rec(0, len(chunks))\n",
    "\n",
    "def test_cluster_chunks_size():\n",
    "    chunks = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    indices = [2, 5, 8, 1, 0, 3, 6, 4, 7]\n",
    "    assert cluster_chunks(chunks, indices, size=1) == ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=2) == ['ab', 'c', 'd', 'ef', 'g', 'hi', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=3) == ['abc', 'def', 'ghi', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=4) == ['abc', 'def', 'ghij']\n",
    "    assert cluster_chunks(chunks, indices, size=7) == ['abc', 'defghij']\n",
    "    assert cluster_chunks(chunks, indices, size=10) == ['abcdefghij']\n",
    "\n",
    "test_cluster_chunks_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magician_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disambiguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n',\n",
       " '\"Wizard (fantasy)\" redirects here. For other uses, see [Wizard\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambiguation\\\\)\").\\n\\n[![](//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-\\nnew.svg/50px-Question_book-new.svg.png)](/wiki/File:Question_book-new.svg)|\\n',\n",
       " 'This article **needs additional citations\\n',\n",
       " 'for[verification](/wiki/Wikipedia:Verifiability \"Wikipedia:Verifiability\")**.\\n']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_semantic(text, size=100):\n",
    "    mini_chunks = chunk_recursive_character_split(text, size)\n",
    "    embeddings = [get_embedding(c) for c in mini_chunks]\n",
    "    similarities = t.cosine_similarity(t.stack(embeddings[:-1]), t.stack(embeddings[1:]))\n",
    "    _, indices = t.sort(similarities)\n",
    "    return cluster_chunks(mini_chunks, indices)\n",
    "\n",
    "chunks = chunk_semantic(document)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy database for our embegginds / chunks pairs\n",
    "def create_db(documents):\n",
    "    chunks = [chunk for document in documents for chunk in chunk_recursive_character_split(document, 100)]\n",
    "    db = t.stack([get_embedding(chunk) for chunk in chunks])\n",
    "    return chunks, db\n",
    "\n",
    "chunks, db = create_db([document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  * _[Dungeons& Dragons](/wiki/Dungeons_%26_Dragons \"Dungeons & Dragons\")_\\n', 'the _[Dungeons& Dragons](/wiki/Dungeons_%26_Dragons \"Dungeons & Dragons\")_\\n']\n",
      "[]\n",
      "['Pyle_The_Enchanter_Merlin.JPG)_The Enchanter Merlin_ , by [Howard\\n', 'Pyle_The_Enchanter_Merlin.JPG/170px-Arthur-\\nPyle_The_Enchanter_Merlin.JPG)](/wiki/File:Arthur-\\n', '\"Mentor\"), with [Merlin](/wiki/Merlin \"Merlin\") from the [_King Arthur_\\n']\n",
      "['series of books by [J. K. Rowling](/wiki/J._K._Rowling \"J. K. Rowling\").\\n\\n', 'the Rings_ or [Lord Voldemort](/wiki/Lord_Voldemort \"Lord Voldemort\") from\\n', 'Lord of the Rings](/wiki/The_Lord_of_the_Rings \"The Lord of the Rings\")_ and\\n']\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k=3, threshold=0.5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = t.cosine_similarity(db, query_embedding)\n",
    "    values, indices = t.topk(similarities, k=k)\n",
    "    indices = indices[values > threshold]\n",
    "    return [chunks[i] for i in indices]\n",
    "\n",
    "print(retrieve('dnd'))\n",
    "print(retrieve('banana'))\n",
    "print(retrieve('merlin the enchanter'))\n",
    "print(retrieve('harry potter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aproximate nearest neighborgs (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### navigable small worlds (NSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
