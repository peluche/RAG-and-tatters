{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG and tatters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threadbare implementation of RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ipywidgets html2text langchain langchain_community sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch as t\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import heapq\n",
    "import requests\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    return html2text.html2text(str(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope():\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    def get_embedding(text):\n",
    "        return t.tensor(embeddings.embed_query(text))\n",
    "    return get_embedding\n",
    "\n",
    "get_embedding = scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option is to ingest full documents, another is to chunk them into smaller pieces. Let's compare different schemes of document chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = wikipedia_to_markdown('https://en.wikipedia.org/wiki/Magician_(fantasy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into fixed size chunks. For simplicity I'll fix the size in characters, but it would be smarter to split by amount of tokens instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magici',\n",
       " 'an_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disamb',\n",
       " 'iguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n\"Wizard (fantasy)\" redirects here. For other uses, see [Wiza',\n",
       " 'rd\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambiguation\\\\)\").\\n\\n[![](//upload.wi']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_fixed_size(text, size):\n",
    "    return [text[i:i+size] for i in range(0, len(text), size)]\n",
    "\n",
    "chunks = chunk_fixed_size(document, 100)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magici',\n",
       " 'iki/Magician_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Mag',\n",
       " '(/wiki/Magi_\\\\(disambiguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n\"Wizard (fantasy)\" redirects here. For o',\n",
       " 'ere. For other uses, see [Wizard\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambi']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_fixed_size_overlap(text, size, overlap):\n",
    "    return [text[i:i+size] for i in range(0, len(text), size - overlap)]\n",
    "\n",
    "chunks = chunk_fixed_size_overlap(document, 100, 10)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive character split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on a hierarchy of specific landmarks (e.g. `'\\n\\n'`, `'\\n'`, `' '`) until we reach the desired size. This is meant to preserve more structure than simple fixed size split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\n',\n",
       " 'For other uses, see [Magician\\n(disambiguation)](/wiki/Magician_\\\\(disambiguation\\\\) \"Magician\\n',\n",
       " '\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disambiguation\\\\)\\n',\n",
       " '\"Magi \\\\(disambiguation\\\\)\").\\n\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_recursive_character_split(text, size, separators=['\\n\\n', '\\n', ' ']):\n",
    "    if len(text) <= size: return [text]\n",
    "    for separator in separators + ['']:\n",
    "        if (index := text[:size].rfind(separator)) != -1:\n",
    "            index += len(separator)\n",
    "            return [text[:index]] + chunk_recursive_character_split(text[index:], size, separators)\n",
    "\n",
    "chunks = chunk_recursive_character_split(document, 100)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document specific splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split based on the document specific grammar (e.g. markdown, HTML, PDF ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\n# The Enigmatic Life of Wizard Eldrath\\n',\n",
       " '\\n## Introduction',\n",
       " '\\nEldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his',\n",
       " ' mysterious powers and secretive nature.\\n',\n",
       " '\\n## Notable Achievements',\n",
       " '\\nEldrath is known for many great deeds, including the discovery of the lost city of Aranthar and',\n",
       " ' the creation of the spell of eternal light.\\n',\n",
       " '```\\ndef eternal_light():\\n    while True:\\n        light_spell.cast()\\n',\n",
       " '```\\nWhich remains one of the most powerful enchantments in existence.\\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_markdown(text, size, offset=0):\n",
    "    ''' piggyback on recursive character split for the demo but it should use a markdown parser '''\n",
    "    separators = [\n",
    "        '\\n# ', '\\n## ', '\\n### ', '\\n#### ', '\\n##### ', '\\n###### ', # headings\n",
    "        '```\\n', '\\n\\n', # blocks\n",
    "        '\\n', '`', '[', ']', '(', ')', '*', '_', # inline\n",
    "        ' ', # words\n",
    "    ]\n",
    "    if len(text) <= size: return [text]\n",
    "    for separator in separators + ['']:\n",
    "        if (index := text[offset:size].rfind(separator)) != -1:\n",
    "            index += offset\n",
    "            return [text[:index]] + chunk_markdown(text[index:], size, offset=len(separator))\n",
    "\n",
    "doc = '''\n",
    "# The Enigmatic Life of Wizard Eldrath\n",
    "\n",
    "## Introduction\n",
    "Eldrath the Wise, a wizard of great renown, has fascinated scholars and adventurers alike with his mysterious powers and secretive nature.\n",
    "\n",
    "## Notable Achievements\n",
    "Eldrath is known for many great deeds, including the discovery of the lost city of Aranthar and the creation of the spell of eternal light.\n",
    "```\n",
    "def eternal_light():\n",
    "    while True:\n",
    "        light_spell.cast()\n",
    "```\n",
    "Which remains one of the most powerful enchantments in existence.\n",
    "'''\n",
    "chunks = chunk_markdown(doc, 100)\n",
    "chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semantic splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the document based on meaning. A naive aproach is to split the document into sentences (here I'll re-use recursive character split) compute their embeddings. Use the embeddings to find topics boundary in the text and merge the rest together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_chunks(chunks, indices, size=500):\n",
    "    ''' cluster chunks such that:\n",
    "      - each cluster is smaller or equal to size\n",
    "      - chunks are clustered according to their relative similarities\n",
    "    '''\n",
    "    indices = [i + 1 for i in indices] # shift to the right\n",
    "\n",
    "    def rec(start, end, idx=0):\n",
    "        # shortcircuit if the entire chunk fits\n",
    "        if sum(len(c) for c in chunks[start:end]) <= size:\n",
    "            return [''.join(chunks[start:end])]\n",
    "        for i in range(idx, len(indices)):\n",
    "            index = indices[i]\n",
    "            if start < index < end:\n",
    "                return rec(start, index, i + 1) + rec(index, end, i + 1)\n",
    "    \n",
    "    return rec(0, len(chunks))\n",
    "\n",
    "def test_cluster_chunks_size():\n",
    "    chunks = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    indices = [2, 5, 8, 1, 0, 3, 6, 4, 7]\n",
    "    assert cluster_chunks(chunks, indices, size=1) == ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=2) == ['ab', 'c', 'd', 'ef', 'g', 'hi', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=3) == ['abc', 'def', 'ghi', 'j']\n",
    "    assert cluster_chunks(chunks, indices, size=4) == ['abc', 'def', 'ghij']\n",
    "    assert cluster_chunks(chunks, indices, size=7) == ['abc', 'defghij']\n",
    "    assert cluster_chunks(chunks, indices, size=10) == ['abcdefghij']\n",
    "\n",
    "test_cluster_chunks_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magicians appearing in fantasy fiction\\n\\nFor other uses, see [Magician\\n(disambiguation)](/wiki/Magician_\\\\(disambiguation\\\\) \"Magician\\n\\\\(disambiguation\\\\)\") and [Magi (disambiguation)](/wiki/Magi_\\\\(disambiguation\\\\)\\n\"Magi \\\\(disambiguation\\\\)\").\\n\\n',\n",
       " '\"Wizard (fantasy)\" redirects here. For other uses, see [Wizard\\n(disambiguation)](/wiki/Wizard_\\\\(disambiguation\\\\) \"Wizard\\n\\\\(disambiguation\\\\)\").\\n\\n[![](//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-\\nnew.svg/50px-Question_book-new.svg.png)](/wiki/File:Question_book-new.svg)|\\n',\n",
       " 'This article **needs additional citations\\n',\n",
       " 'for[verification](/wiki/Wikipedia:Verifiability \"Wikipedia:Verifiability\")**.\\n']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_semantic(text, size=100):\n",
    "    mini_chunks = chunk_recursive_character_split(text, size)\n",
    "    embeddings = [get_embedding(c) for c in mini_chunks]\n",
    "    similarities = t.cosine_similarity(t.stack(embeddings[:-1]), t.stack(embeddings[1:]))\n",
    "    _, indices = t.sort(similarities)\n",
    "    return cluster_chunks(mini_chunks, indices)\n",
    "\n",
    "chunks = chunk_semantic(document)\n",
    "chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy database for our embegginds / chunks pairs\n",
    "def create_db(documents):\n",
    "    chunks = [chunk for document in documents for chunk in chunk_recursive_character_split(document, 100)]\n",
    "    db = t.stack([get_embedding(chunk) for chunk in chunks])\n",
    "    return chunks, db\n",
    "\n",
    "chunks, db = create_db([document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  * _[Dungeons& Dragons](/wiki/Dungeons_%26_Dragons \"Dungeons & Dragons\")_\\n', 'the _[Dungeons& Dragons](/wiki/Dungeons_%26_Dragons \"Dungeons & Dragons\")_\\n']\n",
      "[]\n",
      "['Pyle_The_Enchanter_Merlin.JPG)_The Enchanter Merlin_ , by [Howard\\n', 'Pyle_The_Enchanter_Merlin.JPG/170px-Arthur-\\nPyle_The_Enchanter_Merlin.JPG)](/wiki/File:Arthur-\\n', '\"Mentor\"), with [Merlin](/wiki/Merlin \"Merlin\") from the [_King Arthur_\\n']\n",
      "['series of books by [J. K. Rowling](/wiki/J._K._Rowling \"J. K. Rowling\").\\n\\n', 'the Rings_ or [Lord Voldemort](/wiki/Lord_Voldemort \"Lord Voldemort\") from\\n', 'Lord of the Rings](/wiki/The_Lord_of_the_Rings \"The Lord of the Rings\")_ and\\n']\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k=3, threshold=0.5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = t.cosine_similarity(db, query_embedding)\n",
    "    values, indices = t.topk(similarities, k=k)\n",
    "    indices = indices[values > threshold]\n",
    "    return [chunks[i] for i in indices]\n",
    "\n",
    "print(retrieve('dnd'))\n",
    "print(retrieve('banana'))\n",
    "print(retrieve('merlin the enchanter'))\n",
    "print(retrieve('harry potter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate nearest neighborg (ANN): navigable small worlds (NSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing exhaustive search `O(n)` we can perform an approximate nearest neighborg search using a greedy approach with navigable small world `O(log(n))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, idx, embedding, neighbors=None):\n",
    "        self.idx = idx\n",
    "        self.embedding = embedding\n",
    "        self.neighbors = set(neighbors) if neighbors else set()\n",
    "\n",
    "    def __lt__(self, other): # add a way to sort nodes to make the heapq happy\n",
    "        return self.idx < other.idx\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.idx} {self.neighbors=}'\n",
    "\n",
    "def compute_dist(node, target):\n",
    "    return 1 - t.cosine_similarity(node.embedding, target, dim=0)\n",
    "\n",
    "def greedy_nearest_neighbor(graph, node, target, compute_dist=compute_dist):\n",
    "    steps = []\n",
    "    mini = compute_dist(node, target)\n",
    "    \n",
    "    while True:\n",
    "        best = None\n",
    "        for neighbor in node.neighbors:\n",
    "            neighbor = graph[neighbor]\n",
    "            dist = compute_dist(neighbor, target)\n",
    "            if dist < mini:\n",
    "                mini = dist\n",
    "                best = neighbor\n",
    "        steps.append(node.idx)\n",
    "        if not best: return steps, mini, node\n",
    "        node = best\n",
    "\n",
    "def test_greedy_nearest_neighbor():\n",
    "    def mock_dist(node, target):\n",
    "        ''' use the absolute difference as a distance function to make testing easier '''\n",
    "        return abs(node.embedding - target)\n",
    "\n",
    "    # pretend we have 10 nodes in a line:\n",
    "    # - each node is connected to it's 2 closest neighbors and a random further node\n",
    "    # - values are artificially tweeked to prevent ties\n",
    "    # \n",
    "    # e.g. node 1 is connected to nodes 0, 2, 8\n",
    "    # 0--1--2--3--4--5--6--7--8--9\n",
    "    # ↑  |  ↑                 ↑\n",
    "    # └──┤  |                 |\n",
    "    #    ├──┘                 |\n",
    "    #    └────────────────────┘\n",
    "    graph = [\n",
    "        Node(0, 0,           [1, 2, 7]),\n",
    "        Node(1, 1.1,         [0, 2, 8]),\n",
    "        Node(2, 2.01,        [1, 3, 8]),\n",
    "        Node(3, 3.001,       [2, 4, 6]),\n",
    "        Node(4, 4.0001,      [3, 5, 9]),\n",
    "        Node(5, 5.00001,     [4, 6, 2]),\n",
    "        Node(6, 6.000001,    [5, 7, 2]),\n",
    "        Node(7, 7.0000001,   [6, 8, 0]),\n",
    "        Node(8, 8.00000001,  [7, 9, 5]),\n",
    "        Node(9, 9.000000001, [7, 8, 4]),\n",
    "    ]\n",
    "    # don't test distances because floating numbers are messy and it's not important here\n",
    "    steps, _, nearest = greedy_nearest_neighbor(graph, graph[0], 5, mock_dist)\n",
    "    assert (steps, nearest) == ([0, 7, 6, 5], graph[5])\n",
    "    steps, _, nearest = greedy_nearest_neighbor(graph, graph[0], 8, mock_dist)\n",
    "    assert (steps, nearest) == ([0, 7, 8], graph[8])\n",
    "    steps, _, nearest = greedy_nearest_neighbor(graph, graph[3], 3, mock_dist)\n",
    "    assert (steps, nearest) == ([3], graph[3])\n",
    "    steps, _, nearest = greedy_nearest_neighbor(graph, graph[3], 9, mock_dist)\n",
    "    assert (steps, nearest) == ([3, 6, 7, 8, 9], graph[9])\n",
    "    steps, _, nearest = greedy_nearest_neighbor(graph, graph[5], 1, mock_dist)\n",
    "    assert (steps, nearest) == ([5, 2, 1], graph[1])\n",
    "\n",
    "test_greedy_nearest_neighbor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_k_nearest_neighbors(graph, node, target, k=3, compute_dist=compute_dist):\n",
    "    seen = set([node.idx])\n",
    "    dist = compute_dist(node, target)\n",
    "    nearests = [(-dist, node)] # treat as a maxheap with bounded size `k`\n",
    "    q = [(dist, node)] # minheap\n",
    "    \n",
    "    while q:\n",
    "        _, node = heapq.heappop(q)\n",
    "        # print(f'step: {node}')\n",
    "\n",
    "        for neighbor_idx in node.neighbors:\n",
    "            # print(f'neighbor {neighbor_idx}')\n",
    "            if neighbor_idx in seen: continue\n",
    "            neighbor = graph[neighbor_idx]\n",
    "            dist = compute_dist(neighbor, target)\n",
    "            if len(nearests) < k:\n",
    "                # print(f'push {neighbor_idx}')\n",
    "                heapq.heappush(nearests, (-dist, neighbor))\n",
    "            elif dist < -nearests[0][0]:\n",
    "                # print(f'replace {neighbor_idx}')\n",
    "                heapq.heapreplace(nearests, (-dist, neighbor))\n",
    "            else:\n",
    "                # print(f'continue {neighbor_idx}')\n",
    "                continue\n",
    "            seen.add(neighbor_idx)\n",
    "            heapq.heappush(q, (dist, neighbor))\n",
    "    return [n[1] for n in nearests]\n",
    "\n",
    "def test_greedy_k_nearest_neighbors():\n",
    "    def mock_dist(node, target):\n",
    "        ''' use the absolute difference as a distance function to make testing easier '''\n",
    "        return abs(node.embedding - target)\n",
    "\n",
    "    # pretend we have 10 nodes in a line:\n",
    "    # - each node is connected to it's 2 closest neighbors and a random further node\n",
    "    # - values are artificially tweeked to prevent ties\n",
    "    # \n",
    "    # e.g. node 1 is connected to nodes 0, 2, 8\n",
    "    # 0--1--2--3--4--5--6--7--8--9\n",
    "    # ↑  |  ↑                 ↑\n",
    "    # └──┤  |                 |\n",
    "    #    ├──┘                 |\n",
    "    #    └────────────────────┘\n",
    "    graph = [\n",
    "        Node(0, 0,           [1, 2, 7]),\n",
    "        Node(1, 1.1,         [0, 2, 8]),\n",
    "        Node(2, 2.01,        [1, 3, 8]),\n",
    "        Node(3, 3.001,       [2, 4, 6]),\n",
    "        Node(4, 4.0001,      [3, 5, 9]),\n",
    "        Node(5, 5.00001,     [4, 6, 2]),\n",
    "        Node(6, 6.000001,    [5, 7, 2]),\n",
    "        Node(7, 7.0000001,   [6, 8, 0]),\n",
    "        Node(8, 8.00000001,  [7, 9, 5]),\n",
    "        Node(9, 9.000000001, [7, 8, 4]),\n",
    "    ]\n",
    "    # don't test distances because floating numbers are messy and it's not important here\n",
    "    # all the results should match greedy_nearest_neighbor when k=1\n",
    "    for node in range(10):\n",
    "        for target in range(10):\n",
    "            _, _, nearest = greedy_nearest_neighbor(graph, graph[node], target, compute_dist=mock_dist)\n",
    "            nearests = greedy_k_nearest_neighbors(graph, graph[node], target, k=1, compute_dist=mock_dist)\n",
    "            assert nearests == [nearest]\n",
    "\n",
    "    nearests = greedy_k_nearest_neighbors(graph, graph[0], 5, k=3, compute_dist= mock_dist)\n",
    "    assert set(nearests) == set([graph[4], graph[5], graph[6]])\n",
    "    nearests = greedy_k_nearest_neighbors(graph, graph[5], 5, k=3, compute_dist= mock_dist)\n",
    "    assert set(nearests) == set([graph[4], graph[5], graph[6]])\n",
    "    nearests = greedy_k_nearest_neighbors(graph, graph[0], 0, k=4, compute_dist= mock_dist)\n",
    "    assert set(nearests) == set([graph[0], graph[1], graph[2], graph[3]])\n",
    "    nearests = greedy_k_nearest_neighbors(graph, graph[9], 0, k=4, compute_dist= mock_dist)\n",
    "    assert set(nearests) == set([graph[0], graph[1], graph[2], graph[3]])\n",
    "\n",
    "test_greedy_k_nearest_neighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_neigbors_count=11.925339366515837\n"
     ]
    }
   ],
   "source": [
    "def build_nsw_graph(db, k_near=3, k_random=3):\n",
    "    '''\n",
    "    build a graph by piggybacking on the KNN search and random nodes for far away connections.\n",
    "    the way it is implemented early nodes will have more neighbors (this is not ideal but it'll do for a toy example).\n",
    "    '''\n",
    "    graph = []\n",
    "    # create node and add approximate nearest neighbors\n",
    "    for idx, embedding in enumerate(db):\n",
    "        node = Node(idx, embedding)\n",
    "        graph.append(node)\n",
    "        if not idx: continue\n",
    "        start_node = graph[random.randint(0, idx - 1)]\n",
    "        nearests = greedy_k_nearest_neighbors(graph, start_node, embedding, k=k_near)\n",
    "        for near in nearests:\n",
    "            node.neighbors.add(near.idx)\n",
    "            graph[near.idx].neighbors.add(idx)\n",
    "    # add random connections\n",
    "    for idx in range(len(db)):\n",
    "        for _ in range(k_random):\n",
    "            neighbor = random.randint(0, len(db) - 1)\n",
    "            graph[idx].neighbors.add(neighbor)\n",
    "            graph[neighbor].neighbors.add(idx)\n",
    "    return graph\n",
    "\n",
    "graph = build_nsw_graph(db)\n",
    "average_neigbors_count = sum([len(node.neighbors) for node in graph]) / len(graph)\n",
    "print(f'{average_neigbors_count=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the _[Dungeons& Dragons](/wiki/Dungeons_%26_Dragons \"Dungeons & Dragons\")_\\n']\n",
      "[]\n",
      "['\"Mentor\"), with [Merlin](/wiki/Merlin \"Merlin\") from the [_King Arthur_\\n', 'Pyle_The_Enchanter_Merlin.JPG/170px-Arthur-\\nPyle_The_Enchanter_Merlin.JPG)](/wiki/File:Arthur-\\n', 'Pyle_The_Enchanter_Merlin.JPG)_The Enchanter Merlin_ , by [Howard\\n']\n",
      "['Rowling](/wiki/J._K._Rowling \"J. K. Rowling\")\\'s _Harry Potter_ novels or\\n', 'the Rings_ or [Lord Voldemort](/wiki/Lord_Voldemort \"Lord Voldemort\") from\\n', 'series of books by [J. K. Rowling](/wiki/J._K._Rowling \"J. K. Rowling\").\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "def nsw_retrieve(query, k=3, threshold=0.5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    start_node = graph[random.randint(0, len(graph) - 1)]\n",
    "    nearests = greedy_k_nearest_neighbors(graph, start_node, query_embedding, k=k)\n",
    "    res = []\n",
    "    for near in nearests:\n",
    "        if compute_dist(near, query_embedding) > threshold: continue\n",
    "        res.append(chunks[near.idx])\n",
    "    return res\n",
    "\n",
    "print(nsw_retrieve('dnd'))\n",
    "print(nsw_retrieve('banana'))\n",
    "print(nsw_retrieve('merlin the enchanter'))\n",
    "print(nsw_retrieve('harry potter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_nsw(graph, edge_color='black', ax=None):\n",
    "    if ax is None: _, ax = plt.subplots()\n",
    "\n",
    "    for node in graph:\n",
    "        # plot node\n",
    "        x, y = node.embedding\n",
    "        ax.scatter(x, y, s=1000, zorder=2)\n",
    "        ax.text(x, y, node.idx, color='white', ha='center', va='center')\n",
    "        # plot edges\n",
    "        for neighbor in node.neighbors:\n",
    "            x0, y0 = node.embedding\n",
    "            x1, y1 = graph[neighbor].embedding\n",
    "            ax.plot([x0, x1], [y0, y1], color=edge_color, zorder=1)\n",
    "\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "graph = [\n",
    "    Node(0, (1, 1), [1, 2]),\n",
    "    Node(1, (2, 2), [0, 2]),\n",
    "    Node(2, (1, 2), [0, 1]),\n",
    "    Node(3, (2, 1), [1]),\n",
    "]\n",
    "\n",
    "plot_nsw(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
